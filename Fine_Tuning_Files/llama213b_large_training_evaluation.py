# -*- coding: utf-8 -*-
"""llama213b_large_training_evaluation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z_3UmEDL_erLYi_3cEdVBY1T116VvNFI
"""

!pip install -q -U transformers peft accelerate optimum

!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu117/

pip install wandb

import torch
import json
import os
import pandas as pd
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"

# Open and load the content of the Train_dataset.json file into Train_data
with open("/content/Train_dataset.json") as json_file:
    Train_data = json.load(json_file)

# Open and load the content of the Valid_dataset.json file into Validation_data
with open("/content/Valid_dataset.json") as json_file:
    Validation_data = json.load(json_file)

# Print the first element of Train_data and Validation_data
print(Train_data[0])
print(Validation_data[0])

# Convert Train_data and Validation_data into pandas Series
Train_data = pd.Series(Train_data)
Validation_data = pd.Series(Validation_data)

def formatting_func(example):
    text = f"""<s>[INST] <<SYS>> Answer the question based on the question below, you are a helpful resume points generator.
        [Question]: {example['input']}
        ### [Answer]: {example['output']} [\INST]"""


    return text

# Import necessary modules from transformers library
from transformers import AutoTokenizer, AutoModelForCausalLM, GPTQConfig

# Define quantization configuration with 4 bits and disabling exllama
quantization_config_loading = GPTQConfig(bits=4, disable_exllama=True)

# Specify the model name or path and the model basename
model_name_or_path = "TheBloke/Llama-2-13B-chat-GPTQ"
model_basename = "model"

# Load the pre-trained model for causal language modeling
model = AutoModelForCausalLM.from_pretrained(
    model_name_or_path,
    revision="gptq-4bit-32g-actorder_True",
    use_safetensors=True,
    trust_remote_code=True,
    quantization_config=quantization_config_loading,
    device_map="auto"
)

# Disable caching in the model configuration
model.config.use_cache = False

# Load the tokenizer for the specified model
tokenizer = AutoTokenizer.from_pretrained(
    model_name_or_path,
    padding_side="left",
    add_eos_token=True,
    add_bos_token=True
)

# Set the pad_token to be the same as the eos_token
tokenizer.pad_token = tokenizer.eos_token

# Define a function to generate and tokenize a prompt using a formatting function
def generate_and_tokenize_prompt(prompt):
    return tokenizer(formatting_func(prompt))

tokenised_Train_dataset = Train_data.map(generate_and_tokenize_prompt)
tokenised_valid_dataset = Validation_data.map(generate_and_tokenize_prompt)

import matplotlib.pyplot as plt

def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):
    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]
    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]
    print(len(lengths))

    # Plotting the histogram
    plt.figure(figsize=(10, 6))
    plt.hist(lengths, bins=20, alpha=0.7, color='blue')
    plt.xlabel('Length of input_ids')
    plt.ylabel('Frequency')
    plt.title('Distribution of Lengths of input_ids')
    plt.show()

plot_data_lengths(tokenised_Train_dataset, tokenised_valid_dataset)

max_length = 200

# Define a function to generate and tokenize a prompt using the tokenizer
def generate_and_tokenize_prompt2(prompt):
    # Tokenize the prompt using the specified formatting function and additional configurations
    result = tokenizer(
        formatting_func(prompt),
        truncation=True,
        max_length=max_length,
        padding="max_length"
    )

    # Copy input_ids to labels in the result dictionary
    result["labels"] = result["input_ids"].copy()

    # Return the result
    return result

tokenized_train_dataset = Train_data.map(generate_and_tokenize_prompt2)
tokenized_val_dataset = Validation_data.map(generate_and_tokenize_prompt2)

plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)

"""Pre_Training Eval Prompt"""

eval_prompt = "write some resume points on mars roverteam"

model_input = tokenizer(eval_prompt, return_tensors="pt").to("cuda")

model.eval()
with torch.no_grad():
    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0], skip_special_tokens=True))

from peft import prepare_model_for_kbit_training

model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)

def print_trainable_parameters(model):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}"
    )

from peft import LoraConfig, get_peft_model

# We want to train only these modules
config = LoraConfig(
    r=16,
    lora_alpha=64,
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
        "lm_head",
    ],
    bias="none",
    lora_dropout=0.05,  # Conventional
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, config)
print_trainable_parameters(model)

from accelerate import FullyShardedDataParallelPlugin, Accelerator
from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig

fsdp_plugin = FullyShardedDataParallelPlugin(
    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),
    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),
)

accelerator = Accelerator(fsdp_plugin=fsdp_plugin)

model = accelerator.prepare_model(model)

import wandb, os
wandb.login()

wandb_project = "journal-finetune"
if len(wandb_project) > 0:
    os.environ["WANDB_PROJECT"] = wandb_project

if torch.cuda.device_count() > 1: # If more than 1 GPU
    model.is_parallelizable = True
    model.model_parallel = True

import transformers
from datetime import datetime

project = "journal-finetune"
base_model_name = "llama"
run_name = base_model_name + "-" + project
output_dir = "./" + run_name

trainer = transformers.Trainer(
    model=model,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_val_dataset,
    args=transformers.TrainingArguments(
        output_dir=output_dir,
        warmup_steps=1,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=1,
        gradient_checkpointing=True,
        max_steps=500,
        learning_rate=3e-4, # Want a small lr for finetuning
        fp16=True,
        optim="adamw_hf",
        logging_steps=25,              # When to start reporting loss
        logging_dir="./logs",        # Directory for storing logs
        save_strategy="steps",       # Save the model checkpoint every logging step
        save_steps=25,                # Save checkpoints every 50 steps
        evaluation_strategy="steps", # Evaluate the model every logging step
        eval_steps=25,               # Evaluate and save checkpoints every 50 steps
        do_eval=True,                # Perform evaluation at the end of training
        report_to="wandb",           # Comment this out if you don't want to use weights & baises
        run_name=f"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}"          # Name of the W&B run (optional)
    ),
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
)

model.config.use_cache = False  # silence the warnings. Please re-enable for inference!

trainer.train()

"""We stopped the training in the middle as you see model start to overfit the training data

NOW IMPORTING THE BASE MODEL AND ADDING ADAPTORS
"""

from transformers import AutoTokenizer, AutoModelForCausalLM , GPTQConfig
quantization_config_loading = GPTQConfig(bits=4, disable_exllama=True)
model_name_or_path = "TheBloke/Llama-2-13B-chat-GPTQ"
model_basename = "model"

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True,add_bos_token=True, trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    model_name_or_path,
    revision="gptq-4bit-32g-actorder_True",
    use_safetensors=True,
    trust_remote_code=True,
    quantization_config=quantization_config_loading,
    device_map="auto"
)
model.config.use_cache = False

from peft import PeftModel
ft_model = PeftModel.from_pretrained(model, "/content/llama-journal-finetune/checkpoint-250")

"""EXAMPLES"""

import torch

eval_prompt = " write some resume points on bubble trouble game cs101: # "
model_input = tokenizer(eval_prompt, return_tensors="pt").to("cuda")

ft_model.eval()
with torch.no_grad():
    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=300, repetition_penalty=1.2, top_k=4, do_sample = True)[0], skip_special_tokens=True))

eval_prompt = "write some resume points on Risc pipelined: # "
model_input = tokenizer(eval_prompt, return_tensors="pt").to("cuda")

ft_model.eval()
with torch.no_grad():
    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=300, repetition_penalty=1.2, top_k=4, do_sample = True)[0], skip_special_tokens=True))

eval_prompt = " write some resume points on XLR8: # "
model_input = tokenizer(eval_prompt, return_tensors="pt").to("cuda")

ft_model.eval()
with torch.no_grad():
    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=300, repetition_penalty=1.2, top_k=4, do_sample = True)[0], skip_special_tokens=True))

eval_prompt = " write some resume points on Optimal Placement of Sensor nodes | Supervised Learning Project: # "
model_input = tokenizer(eval_prompt, return_tensors="pt").to("cuda")

ft_model.eval()
with torch.no_grad():
    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=300, repetition_penalty=1.2, top_k=4, do_sample = True)[0], skip_special_tokens=True))

eval_prompt = " write some resume points on Trajectory Simulation of Sentinel-3A Satellite Launch | Spaceflight Mechanics: # "
model_input = tokenizer(eval_prompt, return_tensors="pt").to("cuda")

ft_model.eval()
with torch.no_grad():
    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=300, repetition_penalty=1.2, top_k=4, do_sample = True)[0], skip_special_tokens=True))

# import shutil

# folder_path = '/content/llama-journal-finetune/checkpoint-250'
# shutil.make_archive('/content/folder_download', 'zip', folder_path)

eval_prompt = " write some resume points on Stock trading price prediction: # "
model_input = tokenizer(eval_prompt, return_tensors="pt").to("cuda")

ft_model.eval()
with torch.no_grad():
    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=300, repetition_penalty=1.2, top_k=4, do_sample = True)[0], skip_special_tokens=True))

eval_prompt = " write some resume points on Q-learning RL: # "
model_input = tokenizer(eval_prompt, return_tensors="pt").to("cuda")

ft_model.eval()
with torch.no_grad():
    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=300, repetition_penalty=1.2, top_k=4, do_sample = True)[0], skip_special_tokens=True))

eval_prompt = " write some resume points on Playing tic-tac-toe with AI using minmax algorithm and alpha beta pruning: # "
model_input = tokenizer(eval_prompt, return_tensors="pt").to("cuda")

ft_model.eval()
with torch.no_grad():
    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=300, repetition_penalty=1.2, top_k=4, do_sample = True)[0], skip_special_tokens=True))

eval_prompt = " write some resume points on hf rador: # "
model_input = tokenizer(eval_prompt, return_tensors="pt").to("cuda")

ft_model.eval()
with torch.no_grad():
    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=300, repetition_penalty=1.2, top_k=4, do_sample = True)[0], skip_special_tokens=True))

eval_prompt = " write some resume points on face recognition: # "
model_input = tokenizer(eval_prompt, return_tensors="pt").to("cuda")

ft_model.eval()
with torch.no_grad():
    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=300, repetition_penalty=1.2, top_k=4, do_sample = True)[0], skip_special_tokens=True))

eval_prompt = " write some resume points on Malware Detector-Classifier: # "
model_input = tokenizer(eval_prompt, return_tensors="pt").to("cuda")

ft_model.eval()
with torch.no_grad():
    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=300, repetition_penalty=1.2, top_k=4, do_sample = True)[0], skip_special_tokens=True))

eval_prompt = " write some resume points on Institute Student Mentor (ISMP) and Department Academic Mentor (D-AMP) : # "
model_input = tokenizer(eval_prompt, return_tensors="pt").to("cuda")

ft_model.eval()
with torch.no_grad():
    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=300, repetition_penalty=1.2, top_k=4, do_sample = True)[0], skip_special_tokens=True))

eval_prompt = " write some resume points on Deployed a novel audio-visual dataset generation pipeline to process 7.5 hr of video content in 1 hr to generate 1100+ samples: # "
model_input = tokenizer(eval_prompt, return_tensors="pt").to("cuda")

ft_model.eval()
with torch.no_grad():
    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=300, repetition_penalty=1.2, top_k=4, do_sample = True)[0], skip_special_tokens=True))

