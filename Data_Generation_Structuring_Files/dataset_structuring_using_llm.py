# -*- coding: utf-8 -*-
"""DataSet_Structuring_using_LLM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19OHRTZD-jmArpsVXk-lLPUC3rYzNRzUl
"""

"""
We are going to install the following libraries for this session
1. Langchain
2. CTransformers
"""
!pip install CTransformers
!pip install langchain
!pip install openai

from langchain.llms import CTransformers, OpenAI
from langchain.chains import LLMChain, ConversationChain
from langchain import PromptTemplate
from langchain.chains.router import MultiPromptChain
from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser
from langchain.chains import SimpleSequentialChain, SequentialChain, TransformChain
from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE
from langchain.memory import ConversationBufferMemory

import pandas as pd
import openai
import time
import os

def generate_llmObject_text_completion(model_name="text-davinci-003", temperature = 0):

  # config = {'max_new_tokens': max_new_tokens, 'temperature': temperature, 'context_length': context_length}

  # llm = CTransformers(model = model_path, model_file = model_file, config = config)

  llm = OpenAI(model_name = model_name, temperature = temperature)

  return llm

# def generate_llmObject_text_completion(model_path, model_file, max_new_tokens = 1024, temperature = 0, context_length = 4096):

#   config = {'max_new_tokens': max_new_tokens, 'temperature': temperature, 'context_length': context_length}

#   llm = CTransformers(model = model_path, model_file = model_file, config = config)

#   return llm

model_name = "text-davinci-003"
os.environ["OPENAI_API_KEY"] = "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"#"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"
llm = generate_llmObject_text_completion(model_name = model_name)

# model_path = "TheBloke/Llama-2-7B-chat-GGUF"
# model_file = "llama-2-7b-chat.Q4_0.gguf"
# llm = generate_llmObject_text_completion(model_path = model_path, model_file = model_file)

def simple_chain(prompt, llm, output_key):

  llm_chain = LLMChain(prompt = prompt, llm = llm, output_key = output_key)

  return llm_chain

# from langchain.output_parsers import StructuredOutputParser, ResponseSchema

# response_schemas = [
#     ResponseSchema(name="input", description="Headings of various projects"),
#     ResponseSchema(name="output", description="Corresponding points associated with the headings")
# ]
# output_parser = StructuredOutputParser.from_response_schemas(response_schemas)

# format_instructions = output_parser.get_format_instructions()

def generate_prompt_text_completion(prompt_template, input_variables):

  prompt = PromptTemplate(template = prompt_template, input_variables = input_variables)
  return prompt

#### Step 1: Build a Text Generation Prompt Object --------------------

prompt_template = """
SYSTEM CONTEXT:
You are a resume extracted as text and your task is to separate it into only project headings and their corresponding points associated with it then return them as output

INPUT TASK/QUERY:
{query}

ADDITIONAL INSTRUCTIONS:
See this example for reference

{{
    "input": "HF Radar for Estimation of Ocean Surface Currents",
    "output": [
        "Reviewing the method of obtaining the Doppler spectrum from Bragg scattering by surface waves. This involves determining the location and velocity of the ocean current beneath the surface.",
        "Exploring various algorithms crucial for accurately estimating the location and velocity of ocean currents based on the Doppler spectrum data.",
        "Working on the design and development of a Co-located Orthogonal Loops Antenna for accurate bearing determination of surface waves."
    ]
}}

Do not return the line containing professor's name
Do not write "OUTPUT:" in the 1st line
Return both output in exactly the same JSON format as explained in the example above i.e the heading with the "input" label and corresponding points with "output" label, delimited by ``` at the top and bottom
"""
prompt = generate_prompt_text_completion(prompt_template, input_variables = ["query"])

def read_file_in_chunks(file_path, chunk_size=500):
    """
    Generator to read a file in chunks.

    :param file_path: Path to the text file.
    :param chunk_size: Size of each chunk in characters.
    :return: Yields chunks of the file.
    """
    with open(file_path, 'r', encoding='utf-8') as file:
        while True:
            chunk = file.read(chunk_size)
            if not chunk:
                break
            yield chunk

# def read_file_in_chunks(file_path, chunk_size=1500, overlap_size=100):
#     """
#     Generator to read a file in chunks with overlap between chunks.

#     :param file_path: Path to the text file.
#     :param chunk_size: Size of each chunk in characters.
#     :param overlap_size: Size of the overlap between chunks in characters.
#     :return: Yields chunks of the file with overlap.
#     """
#     with open(file_path, 'r', encoding='utf-8') as file:
#         file.seek(0, 2)  # Move to the end of the file to get its size
#         file_size = file.tell()
#         file.seek(0)  # Move back to the start of the file

#         start = 0
#         end = chunk_size

#         while start < file_size:
#             file.seek(start)
#             chunk = file.read(chunk_size)
#             if not chunk:
#                 break

#             yield chunk

#             start += chunk_size - overlap_size  # Move start for next chunk, considering the overlap
#             end = start + chunk_size

!unzip placement23_RAW.zip

import json
import pandas as pd

for i in range(52):
  file_path = f'/content/output{i+3}.txt'  # Replace with your input file path
  chunks = read_file_in_chunks(file_path)
  for chunk in chunks:
    # Process each chunk here
    # For example, print or send this chunk as part of a query
    llm_chain = simple_chain(prompt = prompt, llm = llm, output_key = "answer")

#------------------------------------------------------

    query = chunk

    start = time.time()

    llm_response = llm_chain.run({"query": query})
    print(llm_response)
    text_data = llm_response
    time.sleep(19)
# File path of the text file to append to
    file_path = "/content/output_JSON_placement_23.txt"

# Open the file in append mode and write the data
    with open(file_path, 'a', encoding='utf-8') as file:
        file.write(text_data + "\n")