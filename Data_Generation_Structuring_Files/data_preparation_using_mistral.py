# -*- coding: utf-8 -*-
"""data-preparation-using-mistral.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C0fnfSmNlq4C_NI7FPww-afYHXLjdUWE
"""

!pip install -q -U transformers
!pip install -q -U accelerate
!pip install -q -U bitsandbytes

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM,pipeline

# Specify the model name path
model_name = '/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1'

# Load the tokenizer for the specified model
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Load the model for causal language modeling
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,  # Use bfloat16 data type
    device_map="auto",  # Automatically manage device placement
    trust_remote_code=True,  # Trust remote code during loading
)

"""Credit:
 - Adapted from https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1
"""

pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        use_cache=True,
        device_map="auto",
        max_length=5000,
        do_sample=True,
        top_k=5,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
)

!pip install -q -U langchain

from langchain import HuggingFacePipeline
llm = HuggingFacePipeline(pipeline=pipeline)

# Import necessary modules from langchain
from langchain.output_parsers import ResponseSchema
from langchain.output_parsers import StructuredOutputParser

# Define a ResponseSchema for the 'name' field
name_schema = ResponseSchema(
    name="title",
    description="Heading of the work or experience"
)

# Define a ResponseSchema for the 'output' field
output_schema = ResponseSchema(
    name="output",
    description="All points related to the heading"
)

# Create a list of ResponseSchema objects
response_schemas = [name_schema, output_schema]

# Output the list of response schemas
response_schemas

output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
output_parser

format_instruction = output_parser.get_format_instructions()
print(format_instruction)

from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate

resume = "RESEARCH EXPERIENCE\n\nHF Radar for Estimation of Ocean Surface Currents Map | Radar (Aug 2020 - Present)\nMaster’s Thesis | Prof. Siddharth Duttagupta\n\n« Reviewed the Method of Obtaining Doppler Spectrum received from Bragg Scattering by surface waves and\nvarious algorithms for estimating its location and velocity of current beneath it\n\n¢ Working on Design of Co-located Orthogonal Loops Antenna for Bearing Determination of surface waves\n\n \n\nVertex Coloring using Oscillators | Neuromorphic (Aug 2019 - Nov 2019)\nSupervised Research Exposition | Prof. Udayan Ganguly\n\n+ Solved Vertex Colouring using Ring Oscillator ,modelled vertex as a oscillator and edge as Coupling Capacitor\n¢ Solved the same problem using Relaxation Oscillator and compared the two methods.\n\nWork EXPERIENCE\n\nAudio Speech Recognition | Meru Cabs, Mumbai (May 2019 - Jul 2019)\nGuide: Jagrat Khandelwal\n\nImplemented Detection of against policy behaviours from Call Recordings of Customers and Drivers by\nrecognising certain words and achieved Accuracy of 0.85 on validation dataset\n\nTrained mapping of 32 cepstral coefficients to phonemes (from HMM) using Fully Connected Nueral Networks\nand phonemes to text"

template = """
        Extract the following details from a description of a resume:


        name: Heading of the work or experience
        output: all the points related to it

        resume description: {resume_description}

        {format_instruction}

"""

prompt = ChatPromptTemplate(
    messages=[
        HumanMessagePromptTemplate.from_template(template)
    ],
    input_variables=["resume_description"],
    partial_variables={"format_instruction": format_instruction},
    output_parser=output_parser # here we add the output parser to the Prompt template
)
messages = prompt.format_messages(resume_description = resume, format_instruction = format_instruction)

prompt.messages

messages

response = llm(messages[0].content)

print(response)